{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomPelletreauDuris/Subjectivity_mining_Assigment_3/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers"
      ],
      "metadata": {
        "id": "A_kMFzbrHB2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 3 \n",
        "\n",
        "###Experimental Setup\n"
      ],
      "metadata": {
        "id": "Es0m-5jcAnWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "RFFA2PufCFSX",
        "outputId": "767f39c3-f55f-4930-964d-844cfbe03d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c806b650-1596-4039-a834-046cfb288e29\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c806b650-1596-4039-a834-046cfb288e29\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving hasoc-train.csv to hasoc-train.csv\n",
            "Saving olid-test.csv to olid-test.csv\n",
            "Saving olid-train.csv to olid-train.csv\n",
            "Saving olid-train-small.csv to olid-train-small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        " \n",
        "df1 = pd.read_csv(io.BytesIO(uploaded['olid-train-small.csv']))\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['olid-test.csv']))\n",
        "df3 = pd.read_csv(io.BytesIO(uploaded['hasoc-train.csv']))\n",
        "print(df1)\n",
        "print(df2)\n",
        "print(df3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKkXUW3ACV2P",
        "outputId": "f689c072-315d-478b-e8ca-7920d00ddef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id                                               text  labels\n",
            "0     24590  @USER @USER @USER @USER @USER Hahah a left tel...       0\n",
            "1     19287  @USER @USER I‚Äôm glad you do babe (I kiss you b...       0\n",
            "2     44676          @USER And I have concerns with‚ÄùDemocrats‚Äù       0\n",
            "3     96110                             @USER AS IT SHOULD BE!       0\n",
            "4     51557  @USER @USER A horrendous act of course. Conser...       1\n",
            "...     ...                                                ...     ...\n",
            "5847  24288  @USER This information is out there and should...       1\n",
            "5848  13930  #MorningJoe the closer it gets to monday the l...       0\n",
            "5849  68792  @USER @USER Long time prisoners will also have...       0\n",
            "5850  19909  @USER @USER It wasn‚Äôt that long ago it was tab...       0\n",
            "5851  33705                    @USER He is so beautiful there.       0\n",
            "\n",
            "[5852 rows x 3 columns]\n",
            "        id                                               text  labels\n",
            "0    15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...       1\n",
            "1    27014  #ConstitutionDay is revered by Conservatives, ...       0\n",
            "2    30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...       0\n",
            "3    13876  #Watching #Boomer getting the news that she is...       0\n",
            "4    60133  #NoPasaran: Unity demo to oppose the far-right...       1\n",
            "..     ...                                                ...     ...\n",
            "855  73439  #DespicableDems lie again about rifles. Dem Di...       1\n",
            "856  25657  #MeetTheSpeakers üôå @USER will present in our e...       0\n",
            "857  67018  3 people just unfollowed me for talking about ...       1\n",
            "858  50665  #WednesdayWisdom Antifa calls the right fascis...       0\n",
            "859  24583      #Kavanaugh typical #liberals , #Democrats URL       0\n",
            "\n",
            "[860 rows x 3 columns]\n",
            "                 id                                               text  labels\n",
            "0        hasoc_en_1  #DhoniKeepsTheGlove | WATCH: Sports Minister K...       0\n",
            "1        hasoc_en_2  @politico No. We should remember very clearly ...       1\n",
            "2        hasoc_en_3  @cricketworldcup Guess who would be the winner...       0\n",
            "3        hasoc_en_4  Corbyn is too politically intellectual for #Bo...       0\n",
            "4        hasoc_en_5  All the best to #TeamIndia for another swimmin...       0\n",
            "...             ...                                                ...     ...\n",
            "5847  hasoc_en_5848  @davidfrum @trueblueusa1 That's cute and all, ...       1\n",
            "5848  hasoc_en_5849  a recession issa comin' #maga #magamyass #fuck...       0\n",
            "5849  hasoc_en_5850  #DoctorsFightBack  Will 'The Mad n Irrational ...       1\n",
            "5850  hasoc_en_5851  #ShiningIndia #educatedindia or more like RUND...       1\n",
            "5851  hasoc_en_5852  Could this be our new Prime Minister?     #Ric...       1\n",
            "\n",
            "[5852 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n"
      ],
      "metadata": {
        "id": "xhBLxOkgHBOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prelimnary analysis \n",
        "\n",
        "Analysis of the BERT hyperparameters."
      ],
      "metadata": {
        "id": "0A7udl4QSvc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We start to set some hyperparameters as the learning rate and the number of epochs to measure differences\n",
        "model_args_1 = ClassificationArgs()\n",
        "model_args_2 = ClassificationArgs()\n",
        "model_args_3 = ClassificationArgs()\n",
        "model_args_4 = ClassificationArgs()\n",
        "model_args_5 = ClassificationArgs()\n",
        "model_args_6 = ClassificationArgs()\n",
        "\n",
        "#Learning rate\n",
        "model_args_1.learning_rate = 1e-5\n",
        "model_args_2.learning_rate = 5e-5\n",
        "model_args_3.learning_rate = 1e-4\n",
        "\n",
        "#Authorizing the overwriting\n",
        "model_args_1.overwrite_output_dir = True\n",
        "model_args_2.overwrite_output_dir = True\n",
        "model_args_3.overwrite_output_dir = True\n",
        "\n",
        "#We start to implement the learning rate 1st to select the best one and then try to change the number of epochs\n",
        "model_Bert_1 = ClassificationModel(\n",
        "    \"bert\", \"bert-base-cased\", args=model_args_1\n",
        ")\n",
        "model_Bert_2 = ClassificationModel(\n",
        "    \"bert\", \"bert-base-cased\", args=model_args_2\n",
        ")\n",
        "model_Bert_3 = ClassificationModel(\n",
        "    \"bert\", \"bert-base-cased\", args=model_args_3\n",
        ")\n",
        "\n",
        "#We train the different models one by one\n",
        "model_Bert_1.train_model(df1, output_dir=\"Bert ID Training 1\")\n",
        "model_Bert_2.train_model(df1, output_dir=\"Bert ID Training 2\")\n",
        "model_Bert_3.train_model(df1, output_dir=\"Bert ID Training 3\")\n",
        "\n",
        "#We evaluate the different models to compare their results and choose the best paramater\n",
        "result_Bert_1, model_outputs_Bert_1, wrong_predictions_Bert_1 = model_Bert_1.eval_model(df2)\n",
        "result_Bert_2, model_outputs_Bert_2, wrong_predictions_Bert_2 = model_Bert_2.eval_model(df2)\n",
        "result_Bert_3, model_outputs_Bert_3, wrong_predictions_Bert_3 = model_Bert_3.eval_model(df2)\n"
      ],
      "metadata": {
        "id": "PGtD2I1l9AU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We display the results and calculate the F1 score to compare them\n",
        "F1_Bert_1 = 2*((result_Bert_1[\"tp\"] / (result_Bert_1[\"tp\"] + result_Bert_1[\"fp\"])) * (result_Bert_1[\"tp\"]/(result_Bert_1[\"tp\"]+result_Bert_1[\"fn\"]))) / ((result_Bert_1[\"tp\"] / (result_Bert_1[\"tp\"] + result_Bert_1[\"fp\"])) + (result_Bert_1[\"tp\"]/(result_Bert_1[\"tp\"]+result_Bert_1[\"fn\"])))\n",
        "F1_Bert_2 = 2*((result_Bert_2[\"tp\"] / (result_Bert_2[\"tp\"] + result_Bert_2[\"fp\"])) * (result_Bert_2[\"tp\"]/(result_Bert_2[\"tp\"]+result_Bert_2[\"fn\"]))) / ((result_Bert_2[\"tp\"] / (result_Bert_2[\"tp\"] + result_Bert_2[\"fp\"])) + (result_Bert_2[\"tp\"]/(result_Bert_2[\"tp\"]+result_Bert_2[\"fn\"])))\n",
        "F1_Bert_3 = 2*((result_Bert_3[\"tp\"] / (result_Bert_3[\"tp\"] + result_Bert_3[\"fp\"])) * (result_Bert_3[\"tp\"]/(result_Bert_3[\"tp\"]+result_Bert_3[\"fn\"]))) / ((result_Bert_3[\"tp\"] / (result_Bert_3[\"tp\"] + result_Bert_3[\"fp\"])) + (result_Bert_3[\"tp\"]/(result_Bert_3[\"tp\"]+result_Bert_3[\"fn\"])))\n"
      ],
      "metadata": {
        "id": "W5emOjyt5TUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(F1_Bert_1)\n",
        "print(F1_Bert_2)\n",
        "print(F1_Bert_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Io4BqQ5rAWt",
        "outputId": "b1d49062-7f63-41ba-c079-2ae148a914c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6887417218543047\n",
            "0.662251655629139\n",
            "0.6755555555555555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nb of epochs with the most efficient learning rate\n",
        "model_args_4.num_train_epochs = 1\n",
        "model_args_4.learning_rate = 1e-5\n",
        "model_args_4.overwrite_output_dir = True\n",
        "\n",
        "model_args_5.num_train_epochs = 2\n",
        "model_args_5.learning_rate = 1e-5\n",
        "model_args_5.overwrite_output_dir = True\n",
        "\n",
        "model_args_6.num_train_epochs = 5\n",
        "model_args_6.learning_rate = 1e-5\n",
        "model_args_6.overwrite_output_dir = True\n",
        "\n",
        "#We implement the models\n",
        "model_Bert_4 = ClassificationModel(\n",
        "    \"bert\", \"bert-base-cased\", args=model_args_4\n",
        ")\n",
        "model_Bert_5 = ClassificationModel(\n",
        "    \"bert\", \"bert-base-cased\", args=model_args_5\n",
        ")\n",
        "model_Bert_6 = ClassificationModel(\n",
        "    \"bert\", \"bert-base-cased\", args=model_args_6\n",
        ")\n",
        "\n",
        "#We train the different models one by one\n",
        "model_Bert_4.train_model(df1, output_dir=\"Bert ID Training 4\")\n",
        "model_Bert_5.train_model(df1, output_dir=\"Bert ID Training 5\")\n",
        "model_Bert_6.train_model(df1, output_dir=\"Bert ID Training 6\")\n",
        "\n",
        "#We evaluate the different models to compare their results and choose the best paramater\n",
        "result_Bert_4, model_outputs_Bert_4, wrong_predictions_Bert_4 = model_Bert_4.eval_model(df2)\n",
        "result_Bert_5, model_outputs_Bert_5, wrong_predictions_Bert_5 = model_Bert_5.eval_model(df2)\n",
        "result_Bert_6, model_outputs_Bert_6, wrong_predictions_Bert_6 = model_Bert_6.eval_model(df2)"
      ],
      "metadata": {
        "id": "Nz6goxGlGbA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We display the 4 result and calculate the F1 score to compare them\n",
        "F1_Bert_4 = 2*((result_Bert_4[\"tp\"] / (result_Bert_4[\"tp\"] + result_Bert_4[\"fp\"])) * (result_Bert_4[\"tp\"]/(result_Bert_4[\"tp\"]+result_Bert_4[\"fn\"]))) / ((result_Bert_4[\"tp\"] / (result_Bert_4[\"tp\"] + result_Bert_4[\"fp\"])) + (result_Bert_4[\"tp\"]/(result_Bert_4[\"tp\"]+result_Bert_4[\"fn\"])))\n",
        "F1_Bert_5 = 2*((result_Bert_5[\"tp\"] / (result_Bert_5[\"tp\"] + result_Bert_5[\"fp\"])) * (result_Bert_5[\"tp\"]/(result_Bert_5[\"tp\"]+result_Bert_5[\"fn\"]))) / ((result_Bert_5[\"tp\"] / (result_Bert_5[\"tp\"] + result_Bert_5[\"fp\"])) + (result_Bert_5[\"tp\"]/(result_Bert_5[\"tp\"]+result_Bert_5[\"fn\"])))\n",
        "F1_Bert_6 = 2*((result_Bert_6[\"tp\"] / (result_Bert_6[\"tp\"] + result_Bert_6[\"fp\"])) * (result_Bert_6[\"tp\"]/(result_Bert_6[\"tp\"]+result_Bert_6[\"fn\"]))) / ((result_Bert_6[\"tp\"] / (result_Bert_6[\"tp\"] + result_Bert_6[\"fp\"])) + (result_Bert_6[\"tp\"]/(result_Bert_6[\"tp\"]+result_Bert_6[\"fn\"])))"
      ],
      "metadata": {
        "id": "YNBybIue6dc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(F1_Bert_4)\n",
        "print(F1_Bert_5)\n",
        "print(F1_Bert_6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmQSihQQzCIg",
        "outputId": "300f9d53-a786-46d4-f1ad-15d52f5e499e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7136563876651982\n",
            "0.7020408163265307\n",
            "0.6789366053169733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###In-domain experiments : \n",
        "After having chosen good hyperparameters, we decided to train the different models with these very same we train the BERT (pre-trained language model), RoBERTa (pre-trained language model) and HateBERT (re-trained transformer model) on the OLID Train dataset (olid-train-small.csv) and evaluate them on the OLID test set (olid_test.csv)"
      ],
      "metadata": {
        "id": "AoPfkkQl7CwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SwUomQQcT2WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We assumed that these preliminary results about the Bert model could apply to most of the others \n",
        "model_args_Roberta = ClassificationArgs()\n",
        "\n",
        "#Learning rate\n",
        "model_args_Roberta.learning_rate = 1e-5\n",
        "#Nb of epochs\n",
        "model_args_Roberta.num_train_epochs = 1\n",
        "#Authorization of overwriting\n",
        "model_args_Roberta.overwrite_output_dir = True\n",
        "\n",
        "#implementation\n",
        "model_Roberta = ClassificationModel(\n",
        "    \"roberta\", \"roberta-base\", args=model_args_Roberta\n",
        ")\n",
        "\n",
        "#training\n",
        "model_Roberta.train_model(df1, output_dir=\"Roberta ID Training\")\n",
        "\n",
        "#We evaluate the model\n",
        "result_Roberta, model_outputs_Roberta, wrong_predictions_Roberta = model_Roberta.eval_model(df2)\n"
      ],
      "metadata": {
        "id": "XZtLL85mD_cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the results of Roberta\n",
        "F1_Roberta = 2*((result_Roberta[\"tp\"] / (result_Roberta[\"tp\"] + result_Roberta[\"fp\"])) * (result_Roberta[\"tp\"]/(result_Roberta[\"tp\"]+result_Roberta[\"fn\"]))) / ((result_Roberta[\"tp\"] / (result_Roberta[\"tp\"] + result_Roberta[\"fp\"])) + (result_Roberta[\"tp\"]/(result_Roberta[\"tp\"]+result_Roberta[\"fn\"])))\n",
        "print(F1_Roberta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg06Jbe0mngS",
        "outputId": "23f9ee22-bf88-4432-f66c-a74115ad1a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7133757961783439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We assumed that these preliminary results about the Bert model could apply to most of the others \n",
        "model_args_HateBert = ClassificationArgs()\n",
        "\n",
        "#Learning rate\n",
        "model_args_HateBert.learning_rate = 1e-5\n",
        "#Nb of epochs\n",
        "model_args_HateBert.num_train_epochs = 5\n",
        "#Authorization of overwriting\n",
        "model_args_HateBert.overwrite_output_dir = True\n",
        "\n",
        "#implementation\n",
        "model_HateBert = ClassificationModel(\n",
        "    \"bert\", \"GroNLP/hateBERT\", args=model_args_HateBert\n",
        ")\n",
        "\n",
        "#training\n",
        "model_HateBert.train_model(df1, output_dir=\"HateBert ID Training\")\n",
        "\n",
        "#We evaluate the model\n",
        "result_HateBert, model_outputs_HateBert, wrong_predictions_HateBert = model_HateBert.eval_model(df2)\n"
      ],
      "metadata": {
        "id": "aCbHfMFTEBMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the results of HateBert\n",
        "F1_HateBert = 2*((result_HateBert[\"tp\"] / (result_HateBert[\"tp\"] + result_HateBert[\"fp\"])) * (result_HateBert[\"tp\"]/(result_HateBert[\"tp\"]+result_HateBert[\"fn\"]))) / ((result_HateBert[\"tp\"] / (result_HateBert[\"tp\"] + result_HateBert[\"fp\"])) + (result_HateBert[\"tp\"]/(result_HateBert[\"tp\"]+result_HateBert[\"fn\"])))\n",
        "print(F1_HateBert)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbVnuQYcIsMg",
        "outputId": "0f6ceecb-13fd-4224-aecf-43f4592e113c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6652542372881356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The results :\n"
      ],
      "metadata": {
        "id": "QKi0vpAfJudu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"BERT MODEL : \")\n",
        "print(model_Bert_4)\n",
        "print(\"precision : \")\n",
        "print((result_Bert_4[\"tp\"] / (result_Bert_4[\"tp\"] + result_Bert_4[\"fp\"])))\n",
        "print(\"recall : \") \n",
        "print((result_Bert_4[\"tp\"]/(result_Bert_4[\"tp\"]+result_Bert_4[\"fn\"])))\n",
        "F1_Bert_4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foEQ99o7J6iO",
        "outputId": "df593320-83f9-4a15-ae10-c04dbfb012e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT MODEL : \n",
            "<simpletransformers.classification.classification_model.ClassificationModel object at 0x7fe080b58f90>\n",
            "precision : \n",
            "0.7570093457943925\n",
            "recall : \n",
            "0.675\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7136563876651982"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"RoBERTa MODEL : \")\n",
        "print(result_Roberta)\n",
        "print((result_Roberta[\"tp\"] / (result_Roberta[\"tp\"] + result_Roberta[\"fp\"])))\n",
        "print((result_Roberta[\"tp\"]/(result_Roberta[\"tp\"]+result_Roberta[\"fn\"])))\n",
        "F1_Roberta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az1HYuA3L-In",
        "outputId": "e7ff9879-43ff-434c-8446-d7cdaba25210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa MODEL : \n",
            "{'mcc': 0.6055532970245479, 'tp': 168, 'tn': 557, 'fp': 63, 'fn': 72, 'auroc': 0.8911458333333333, 'auprc': 0.7699373471837916, 'eval_loss': 0.3817753968415437}\n",
            "0.7272727272727273\n",
            "0.7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7133757961783439"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"HateBERT MODEL : \")\n",
        "print(result_HateBert)\n",
        "print((result_HateBert[\"tp\"] / (result_HateBert[\"tp\"] + result_HateBert[\"fp\"])))\n",
        "print((result_HateBert[\"tp\"]/(result_HateBert[\"tp\"]+result_HateBert[\"fn\"])))\n",
        "F1_HateBert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w06H_VTYL_cv",
        "outputId": "95010087-f9b8-4a3e-acbf-a765384e96ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HateBERT MODEL : \n",
            "{'mcc': 0.5388487952673509, 'tp': 157, 'tn': 545, 'fp': 75, 'fn': 83, 'auroc': 0.8486995967741936, 'auprc': 0.7443323453403545, 'eval_loss': 0.6252115854510555}\n",
            "0.6767241379310345\n",
            "0.6541666666666667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6652542372881356"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ERROR ANALYSIS part\n",
        "\n",
        "Here we can take a look at the different wrong predictions that have been made and analyse the causes that could explains these misunderstandings.\n",
        "\n"
      ],
      "metadata": {
        "id": "fLvtj8NIR5uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_predictions_Bert_4"
      ],
      "metadata": {
        "id": "ag9_H3V3ojjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_predictions_Roberta"
      ],
      "metadata": {
        "id": "-rHMC8YmojTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_predictions_HateBert"
      ],
      "metadata": {
        "id": "ip4swwBxoew6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cross-domain experiments : \n",
        "We train the BERT (pre-trained language model), RoBERTa (pre-trained language model) and HateBERT (re-trained transformer model) on the HASOC Train dataset (olid-train-small.csv) and evaluate them on the OLID test set (olid_test.csv)"
      ],
      "metadata": {
        "id": "ar_2hrihBAqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert\n",
        "#We train the Bert model on the other dataset (Cross Domain).\n",
        "model_Bert_4.train_model(df3, output_dir=\"Bert CD Training\")\n",
        "\n",
        "#And evaluate it on the first dataset\n",
        "result_Bert_CD, model_outputs_Bert_CD, wrong_predictions_Bert_CD = model_Bert_4.eval_model(df2)\n",
        "\n",
        "#Roberta\n",
        "#training\n",
        "model_Roberta.train_model(df3, output_dir=\"Roberta CD Training\")\n",
        "#eval\n",
        "result_Roberta_CD, model_outputs_Roberta_CD, wrong_predictions_Roberta_CD = model_Roberta.eval_model(df2)\n",
        "\n",
        "#HateBert\n",
        "#training\n",
        "model_HateBert.train_model(df3, output_dir=\"HateBert CD Training\")\n",
        "#eval\n",
        "result_HateBert_CD, model_outputs_HateBert_CD, wrong_predictions_HateBert_CD = model_HateBert.eval_model(df2)"
      ],
      "metadata": {
        "id": "kx2jyq_qMeMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F1_Bert_CD = 2*((result_Bert_CD[\"tp\"] / (result_Bert_CD[\"tp\"] + result_Bert_CD[\"fp\"])) * (result_Bert_CD[\"tp\"]/(result_Bert_CD[\"tp\"]+result_Bert_CD[\"fn\"]))) / ((result_Bert_CD[\"tp\"] / (result_Bert_CD[\"tp\"] + result_Bert_CD[\"fp\"])) + (result_Bert_CD[\"tp\"]/(result_Bert_CD[\"tp\"]+result_Bert_CD[\"fn\"])))\n",
        "F1_Roberta_CD = 2*((result_Roberta_CD[\"tp\"] / (result_Roberta_CD[\"tp\"] + result_Roberta_CD[\"fp\"])) * (result_Roberta_CD[\"tp\"]/(result_Roberta_CD[\"tp\"]+result_Roberta_CD[\"fn\"]))) / ((result_Roberta_CD[\"tp\"] / (result_Roberta_CD[\"tp\"] + result_Roberta_CD[\"fp\"])) + (result_Roberta_CD[\"tp\"]/(result_Roberta_CD[\"tp\"]+result_Roberta_CD[\"fn\"])))\n",
        "F1_HateBert_CD = 2*((result_HateBert_CD[\"tp\"] / (result_HateBert_CD[\"tp\"] + result_HateBert_CD[\"fp\"])) * (result_HateBert_CD[\"tp\"]/(result_HateBert_CD[\"tp\"]+result_HateBert_CD[\"fn\"]))) / ((result_HateBert_CD[\"tp\"] / (result_HateBert_CD[\"tp\"] + result_HateBert_CD[\"fp\"])) + (result_HateBert_CD[\"tp\"]/(result_HateBert_CD[\"tp\"]+result_HateBert_CD[\"fn\"])))"
      ],
      "metadata": {
        "id": "zU_OMFKR82Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The results"
      ],
      "metadata": {
        "id": "qsJf2VKxJtLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"BERT MODEL : \")\n",
        "print(result_Bert_CD)\n",
        "print((result_Bert_CD[\"tp\"] / (result_Bert_CD[\"tp\"] + result_Bert_CD[\"fp\"])))\n",
        "print((result_Bert_CD[\"tp\"]/(result_Bert_CD[\"tp\"]+result_Bert_CD[\"fn\"])))\n",
        "print(F1_Bert_CD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9guoMSeOU8A",
        "outputId": "aaa7f1b6-a76d-4362-e92c-a2b7c4d9f0f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT MODEL : \n",
            "{'mcc': 0.5638424840096687, 'tp': 135, 'tn': 583, 'fp': 37, 'fn': 105, 'auroc': 0.8687970430107527, 'auprc': 0.7442916334935982, 'eval_loss': 0.4226069273772063}\n",
            "0.7848837209302325\n",
            "0.5625\n",
            "0.6553398058252426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"RoBERTa MODEL : \")\n",
        "print(result_Roberta_CD)\n",
        "print((result_Roberta_CD[\"tp\"] / (result_Roberta_CD[\"tp\"] + result_Roberta_CD[\"fp\"])))\n",
        "print((result_Roberta_CD[\"tp\"]/(result_Roberta_CD[\"tp\"]+result_Roberta_CD[\"fn\"])))\n",
        "print(F1_Roberta_CD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc6DvwJ7OX2f",
        "outputId": "89760b8a-762d-4f1f-f0fa-ee86d3d19c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa MODEL : \n",
            "{'mcc': 0.5802155091063779, 'tp': 138, 'tn': 585, 'fp': 35, 'fn': 102, 'auroc': 0.869331317204301, 'auprc': 0.7582790129891647, 'eval_loss': 0.41459055300112124}\n",
            "0.7976878612716763\n",
            "0.575\n",
            "0.6682808716707022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"HateBERT MODEL : \")\n",
        "print(result_HateBert_CD)\n",
        "print((result_HateBert_CD[\"tp\"] / (result_HateBert_CD[\"tp\"] + result_HateBert_CD[\"fp\"])))\n",
        "print((result_HateBert_CD[\"tp\"]/(result_HateBert_CD[\"tp\"]+result_HateBert_CD[\"fn\"])))\n",
        "print(F1_HateBert_CD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBN3lKqYOX6G",
        "outputId": "1b3cf8ae-7fa2-4026-d271-51f7d26dc4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HateBERT MODEL : \n",
            "{'mcc': 0.41705109903513105, 'tp': 145, 'tn': 509, 'fp': 111, 'fn': 95, 'auroc': 0.7831720430107526, 'auprc': 0.6423770346373658, 'eval_loss': 0.6312080489264594}\n",
            "0.56640625\n",
            "0.6041666666666666\n",
            "0.5846774193548387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Error analysis\n",
        " Analysis of the wrong predictions by the three models in Cross-domain experiment."
      ],
      "metadata": {
        "id": "YZhSE6HuUU-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_predictions_Bert_CD"
      ],
      "metadata": {
        "id": "29_ULyO7UuOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_predictions_Roberta_CD"
      ],
      "metadata": {
        "id": "dMNAibkaUsM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_predictions_HateBert_CD"
      ],
      "metadata": {
        "id": "ZZrYDFteUS2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Other models experiment \n",
        "\n",
        "We will now explore other techniques such as CNNs, LSTMs and BiLSTMs to compare."
      ],
      "metadata": {
        "id": "ZxbGQ-tD5d7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "koqmJCWAQ1jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from keras.layers import LSTM\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "traindata=pd.read_csv (r'olid-train-small.csv')\n",
        "train_x=traindata['text'].tolist()\n",
        "train_y=traindata['labels'].tolist()\n",
        "testdata=pd.read_csv (r'olid-test.csv')\n",
        "test_x=testdata['text'].tolist()\n",
        "test_y=testdata['labels'].tolist()\n",
        "\n",
        "text_clf= Pipeline([\n",
        "    ('vect',CountVectorizer()),\n",
        "    ('clf', MLPClassifier(max_iter=300))])\n",
        "\n",
        "clf = text_clf.fit(train_x, train_y)\n"
      ],
      "metadata": {
        "id": "0oAGaHBpQ1jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions = text_clf.predict(test_x).tolist()\n",
        "\n",
        "print(classification_report(predictions,test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ef1c41-4e13-4746-d5df-d900fb3ff005",
        "id": "46_CGKm6Q1jl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       606\n",
            "           1       0.55      0.52      0.53       254\n",
            "\n",
            "    accuracy                           0.73       860\n",
            "   macro avg       0.67      0.67      0.67       860\n",
            "weighted avg       0.73      0.73      0.73       860\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other models on cross domain data\n"
      ],
      "metadata": {
        "id": "VVl3AlcpWnzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traindata=pd.read_csv (r'hasoc-train.csv')\n",
        "train_x=traindata['text'].tolist()\n",
        "train_y=traindata['labels'].tolist()\n",
        "testdata=pd.read_csv (r'olid-test.csv')\n",
        "test_x=testdata['text'].tolist()\n",
        "test_y=testdata['labels'].tolist()\n",
        "\n",
        "text_clf= Pipeline([\n",
        "    ('vect',CountVectorizer()),\n",
        "    ('clf', MLPClassifier(max_iter=300))])\n",
        "\n",
        "clf2 = text_clf.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "U3pDeoSXUizI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions2 = text_clf.predict(test_x).tolist()\n",
        "\n",
        "print(classification_report(predictions2,test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhxF_hmZVBPK",
        "outputId": "06a5f41a-582e-4371-c3e9-93968768fc18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.78      0.76       588\n",
            "           1       0.45      0.40      0.43       272\n",
            "\n",
            "    accuracy                           0.66       860\n",
            "   macro avg       0.60      0.59      0.59       860\n",
            "weighted avg       0.65      0.66      0.65       860\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4TgrMOU4BPmI"
      }
    }
  ]
}